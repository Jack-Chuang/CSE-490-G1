{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-XMbbmJ4x-98",
        "ypodd5NXeigP",
        "px-nGLkXeMS1"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "EXhVcYcysidP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"your directory/CSE-490-G1/final project/codes\"\n",
        "!pip install -r requirements.txt\n",
        "%cd torchsearchsorted\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "33ZNDBpnsjpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download pretrained weight and dataset (No need for our own collected data)"
      ],
      "metadata": {
        "id": "-XMbbmJ4x-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'your downloaded logs.zip' -d ./"
      ],
      "metadata": {
        "id": "CYGYNBP-x-UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'your downloaded data.zip' -d ./"
      ],
      "metadata": {
        "id": "PQBYPpVbzr7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "This section lets you train your own D-NeRF or NeRF model. The configuration is defined by D-NeRF/configs/your_config.txt (for details of arguements please see **run_dnerf.py**)."
      ],
      "metadata": {
        "id": "wsronU_Twp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change to your config\n",
        "!python run_dnerf.py --config configs/kim_move1.txt"
      ],
      "metadata": {
        "id": "5WrR3iJCx5SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test \n",
        "\n",
        "This section allows you to test the trained D-NeRF or NeRF model.\n",
        "You can synthesize novel views by running this script.\n",
        "\n",
        "The details of arguements are described in the **test_dnerf.py** file."
      ],
      "metadata": {
        "id": "Lmub7uuodry_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_dnerf.py --config configs/jack_close1.txt --render_test --renderonly_render_times=20 --fps=5 --start_select=0 --end_select=320 --video_name=jack_psnr_new.mp4 --test_lookat=0 --test_render_dir='jack_dnerf_psnr' --fixed_renderpose=0"
      ],
      "metadata": {
        "id": "KNaCPMKdduY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43039a2f-ede4-4276-dfe0-e36c0ca02d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 3993755648 bytes == 0x55f46c6c2000 @  0x7f0c9b9e01e7 0x7f0c8fc5346e 0x7f0c8fca3c7b 0x7f0c8fca3d18 0x7f0c8fd4b010 0x7f0c8fd4b73c 0x7f0c8fd4b85d 0x55f378ced749 0x7f0c8fc90ef7 0x55f378ceb437 0x55f378ceb240 0x55f378d5e973 0x55f378d599ee 0x55f378cecbda 0x55f378d5b737 0x55f378d59ced 0x55f378c2beb0 0x7f0c8fc90ef7 0x55f378ceb437 0x55f378ceb240 0x55f378d5e973 0x55f378d599ee 0x55f378cecbda 0x55f378d5ed00 0x55f378d59ced 0x55f378cecbda 0x55f378d5b737 0x55f378d599ee 0x55f378cecbda 0x55f378d5b737 0x55f378d59ced\n",
            "Loaded image data (540, 960, 3, 321) [540.         960.         842.57913105]\n",
            "Loaded ./data/jack_close1 0.264175094272201 65.00208395488232\n",
            "tcmalloc: large alloc 1996881920 bytes == 0x55f37e26c000 @  0x7f0c9b9e01e7 0x7f0c8fc5346e 0x7f0c8fca3c7b 0x7f0c8fca3ebe 0x7f0c8fd3c887 0x55f378ceb4b0 0x55f378ddce1d 0x55f378d5ee99 0x55f378d599ee 0x55f378cecbda 0x55f378d5b737 0x55f378d59ced 0x55f378cecbda 0x55f378d5a915 0x55f378d599ee 0x55f378d596f3 0x55f378e234c2 0x55f378e2383d 0x55f378e236e6 0x55f378dfb163 0x55f378dfae0c 0x7f0c9a7cabf7 0x55f378dfacea\n",
            "Data:\n",
            "(320, 3, 5) (320, 540, 960, 3) (320, 2)\n",
            "HOLDOUT view is 307\n",
            "tcmalloc: large alloc 1990656000 bytes == 0x55f3f52cc000 @  0x7f0c9b9e01e7 0x7f0c8fc5346e 0x7f0c8fca3c7b 0x7f0c8fca3ebe 0x7f0c8fd3c887 0x55f378ceb4b0 0x55f378ddce1d 0x55f378d5ee99 0x55f378d599ee 0x55f378cecbda 0x55f378d5b737 0x55f378d59ced 0x55f378cecbda 0x55f378d5a915 0x55f378d599ee 0x55f378d596f3 0x55f378e234c2 0x55f378e2383d 0x55f378e236e6 0x55f378dfb163 0x55f378dfae0c 0x7f0c9a7cabf7 0x55f378dfacea\n",
            "Loaded llff (320, 540, 960, 3) (120, 3, 5) [540.     960.     842.5791] ./data/jack_close1\n",
            "Auto LLFF holdout, 8\n",
            "DEFINING BOUNDS\n",
            "NEAR FAR 0.06078765392303467 16.619129180908203\n",
            "i_train:  [  0   2   3   4   5   6   7   8  10  11  12  13  14  15  16  18  19  20\n",
            "  21  22  23  24  26  27  28  29  30  31  32  34  35  36  37  38  39  40\n",
            "  42  43  44  45  46  47  48  50  51  52  53  54  55  56  58  59  60  61\n",
            "  62  63  64  66  67  68  69  70  71  72  74  75  76  77  78  79  80  82\n",
            "  83  84  85  86  87  88  90  91  92  93  94  95  96  98  99 100 101 102\n",
            " 103 104 106 107 108 109 110 111 112 114 115 116 117 118 119 120 122 123\n",
            " 124 125 126 127 128 130 131 132 133 134 135 136 138 139 140 141 142 143\n",
            " 144 146 147 148 149 150 151 152 154 155 156 157 158 159 160 162 163 164\n",
            " 165 166 167 168 170 171 172 173 174 175 176 178 179 180 181 182 183 184\n",
            " 186 187 188 189 190 191 192 194 195 196 197 198 199 200 202 203 204 205\n",
            " 206 207 208 210 211 212 213 214 215 216 218 219 220 221 222 223 224 226\n",
            " 227 228 229 230 231 232 234 235 236 237 238 239 240 242 243 244 245 246\n",
            " 247 248 250 251 252 253 254 255 256 258 259 260 261 262 263 264 266 267\n",
            " 268 269 270 271 272 274 275 276 277 278 279 280 282 283 284 285 286 287\n",
            " 288 290 291 292 293 294 295 296 298 299 300 301 302 303 304 306 307 308\n",
            " 309 310 311 312 314 315 316 317 318 319]\n",
            "i_val, i_test [  1   9  17  25  33  41  49  57  65  73  81  89  97 105 113 121 129 137\n",
            " 145 153 161 169 177 185 193 201 209 217 225 233 241 249 257 265 273 281\n",
            " 289 297 305 313] [  1   9  17  25  33  41  49  57  65  73  81  89  97 105 113 121 129 137\n",
            " 145 153 161 169 177 185 193 201 209 217 225 233 241 249 257 265 273 281\n",
            " 289 297 305 313]\n",
            "NeRF type selected: direct_temporal\n",
            "Found ckpts ['./logs/jack_close1/050000.tar', './logs/jack_close1/100000.tar', './logs/jack_close1/110000.tar', './logs/jack_close1/120000.tar', './logs/jack_close1/130000.tar', './logs/jack_close1/140000.tar', './logs/jack_close1/150000.tar', './logs/jack_close1/160000.tar', './logs/jack_close1/170000.tar', './logs/jack_close1/180000.tar', './logs/jack_close1/190000.tar', './logs/jack_close1/200000.tar', './logs/jack_close1/210000.tar', './logs/jack_close1/220000.tar', './logs/jack_close1/230000.tar', './logs/jack_close1/240000.tar', './logs/jack_close1/250000.tar', './logs/jack_close1/260000.tar', './logs/jack_close1/270000.tar', './logs/jack_close1/280000.tar', './logs/jack_close1/290000.tar', './logs/jack_close1/300000.tar', './logs/jack_close1/310000.tar', './logs/jack_close1/320000.tar', './logs/jack_close1/330000.tar', './logs/jack_close1/340000.tar', './logs/jack_close1/350000.tar', './logs/jack_close1/360000.tar', './logs/jack_close1/370000.tar', './logs/jack_close1/380000.tar', './logs/jack_close1/390000.tar', './logs/jack_close1/400000.tar']\n",
            "Reloading from ./logs/jack_close1/400000.tar\n",
            "RENDER ONLY\n",
            "  0% 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "100% 40/40 [26:07<00:00, 39.19s/it]\n",
            "psnr_list: [tensor(25.9828, device='cpu'), tensor(26.6821, device='cpu'), tensor(24.9651, device='cpu'), tensor(21.2046, device='cpu'), tensor(22.7582, device='cpu'), tensor(24.7205, device='cpu'), tensor(23.7021, device='cpu'), tensor(24.2057, device='cpu'), tensor(25.9734, device='cpu'), tensor(26.4808, device='cpu'), tensor(26.9434, device='cpu'), tensor(28.8115, device='cpu'), tensor(26.6548, device='cpu'), tensor(23.1759, device='cpu'), tensor(21.8988, device='cpu'), tensor(21.8461, device='cpu'), tensor(21.6200, device='cpu'), tensor(24.7181, device='cpu'), tensor(26.0101, device='cpu'), tensor(27.9811, device='cpu'), tensor(27.2176, device='cpu'), tensor(24.7109, device='cpu'), tensor(22.3820, device='cpu'), tensor(24.1643, device='cpu'), tensor(22.8736, device='cpu'), tensor(24.5868, device='cpu'), tensor(27.4533, device='cpu'), tensor(27.6670, device='cpu'), tensor(28.4733, device='cpu'), tensor(27.7721, device='cpu'), tensor(23.4376, device='cpu'), tensor(21.1744, device='cpu'), tensor(22.1630, device='cpu'), tensor(21.5335, device='cpu'), tensor(24.9908, device='cpu'), tensor(27.2202, device='cpu'), tensor(27.9916, device='cpu'), tensor(26.0839, device='cpu'), tensor(22.0168, device='cpu'), tensor(21.9427, device='cpu')]\n",
            "Done rendering ./logs/test_render/jack_dnerf_psnr\n",
            "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (540, 960) to (544, 960) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitatively evaluate trained model"
      ],
      "metadata": {
        "id": "ypodd5NXeigP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lpips\n",
        "import os\n",
        "import imageio\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ui7VuGGPekaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Square Error\n",
        "class MSE(object):\n",
        "    def __call__(self, pred, gt):\n",
        "        return torch.mean((pred - gt) ** 2)"
      ],
      "metadata": {
        "id": "A5osP0R0evuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peak Signal to Noise Ratio\n",
        "class PSNR(object):\n",
        "    def __call__(self, pred, gt):\n",
        "        mse = torch.mean((pred - gt) ** 2)\n",
        "        return 10 * torch.log10(1 / mse)"
      ],
      "metadata": {
        "id": "tw7Q_VezewSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# structural similarity index\n",
        "class SSIM(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "    '''\n",
        "    def gaussian(self, w_size, sigma):\n",
        "        gauss = torch.Tensor([math.exp(-(x - w_size//2)**2/float(2*sigma**2)) for x in range(w_size)])\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    def create_window(self, w_size, channel=1):\n",
        "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
        "        return window\n",
        "\n",
        "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            w_size : int, default 11\n",
        "            size_average : boolean, default True\n",
        "            full : boolean, default False\n",
        "        return ssim, larger the better\n",
        "        \"\"\"\n",
        "        # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "        if torch.max(y_pred) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(y_pred) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "\n",
        "        padd = 0\n",
        "        (_, channel, height, width) = y_pred.size()\n",
        "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
        "\n",
        "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
        "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        sigma1_sq = F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "        C1 = (0.01 * L) ** 2\n",
        "        C2 = (0.03 * L) ** 2\n",
        "\n",
        "        v1 = 2.0 * sigma12 + C2\n",
        "        v2 = sigma1_sq + sigma2_sq + C2\n",
        "        cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "        if size_average:\n",
        "            ret = ssim_map.mean()\n",
        "        else:\n",
        "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "        if full:\n",
        "            return ret, cs\n",
        "        return ret"
      ],
      "metadata": {
        "id": "tg8BdYGie0or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learned Perceptual Image Patch Similarity\n",
        "class LPIPS(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.model = lpips.LPIPS(net='vgg').cuda()\n",
        "\n",
        "    def __call__(self, y_pred, y_true, normalized=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            normalized : change [0,1] => [-1,1] (default by LPIPS)\n",
        "        return LPIPS, smaller the better\n",
        "        \"\"\"\n",
        "        if normalized:\n",
        "            y_pred = y_pred * 2.0 - 1.0\n",
        "            y_true = y_true * 2.0 - 1.0\n",
        "        error =  self.model.forward(y_pred, y_true)\n",
        "        return torch.mean(error)"
      ],
      "metadata": {
        "id": "8k0nG0Sfe1FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_images_in_dir(imgs_dir):\n",
        "    imgs = []\n",
        "    fnames = os.listdir(imgs_dir)\n",
        "    fnames.sort()\n",
        "    for fname in fnames:\n",
        "        if fname == \"000.png\" :  # ignore canonical space, only evalute real scene\n",
        "            continue\n",
        "            \n",
        "        img_path = os.path.join(imgs_dir, fname)\n",
        "        img = imageio.imread(img_path)\n",
        "        img = (np.array(img) / 255.).astype(np.float32)\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "        imgs.append(img)\n",
        "    \n",
        "    imgs = np.stack(imgs)       \n",
        "    return imgs\n",
        "\n",
        "def estim_error(estim, gt):\n",
        "    errors = dict()\n",
        "    metric = MSE()\n",
        "    errors[\"mse\"] = metric(estim, gt).item()\n",
        "    metric = PSNR()\n",
        "    errors[\"psnr\"] = metric(estim, gt).item()\n",
        "    metric = SSIM()\n",
        "    errors[\"ssim\"] = metric(estim, gt).item()\n",
        "    metric = LPIPS()\n",
        "    errors[\"lpips\"] = metric(estim, gt).item()\n",
        "    return errors\n",
        "\n",
        "def save_error(errors, save_dir):\n",
        "    save_path = os.path.join(save_dir, \"metrics.txt\")\n",
        "    f = open(save_path,\"w\")\n",
        "    f.write( str(errors) )\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "r6jKbLjJe4y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_dir = \"the root folder containing synthesized images (estim) and ground truth images (gt)\"\n",
        "\n",
        "estim_dir = os.path.join(files_dir, \"estim\")\n",
        "gt_dir = os.path.join(files_dir, \"gt\")\n",
        "\n",
        "estim = read_images_in_dir(estim_dir)\n",
        "gt = read_images_in_dir(gt_dir)\n",
        "\n",
        "estim = torch.Tensor(estim).cuda()\n",
        "gt = torch.Tensor(gt).cuda()\n",
        "\n",
        "errors = estim_error(estim, gt)\n",
        "save_error(errors, files_dir)\n",
        "print(errors)"
      ],
      "metadata": {
        "id": "d5mtUBd0e8KZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}